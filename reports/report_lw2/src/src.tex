\section{Описание}
Требуется разработать программу токенизатор, которая будет разбивать текст на токены для дальнейшей обработки. В качестве разделителя довольно эффективно использовать знаки препинания, однако некоторые токены таким образом будут утрачены (например сокращения). 

Так же я добавил и другие разделяющие символы, которые были выявлены в процессе анализа исходного текста. Благодаря им, качество токенизации существенно улучшилось. Эти и другие символы можно увидеть в исходном коде.

После получения токена из текста необходимо проверить, что данный токен не является числом и оно не содержится в стоп-словах. После всех проверок токен будет добавлен в файл.

13202 документа токенизируются за 12 минут. Время работы программы можно сократить, если уменьшить количество чтений с диска. Так же, нужно уменьшить количество записей на диск.

\pagebreak

\section{Исходный код}

\lstset{ %
   texcl=true,
   escapeinside={//}{\^^M},
}

\begin{lstlisting}[language=C++]

#include <iostream>
#include <regex>
#include <boost/filesystem.hpp>
#include <boost/range/iterator_range.hpp>
#include <fstream>
#include <algorithm>

#include <utils.hpp>
#include <common.h>
#include "stop_words.h"
#include <vector>

std::map<std::string, uint64_t> frequency;

namespace fs = boost::filesystem;

bool is_token_correct(const std::string& low_str)
{
	if (stop_words.find(low_str) != stop_words.end()) // stop words
		return false;

	if (utils::is_digit(low_str))
		return false;

	return true;
}

void tokenize_text(const fs::path& input, const fs::path& output)
{
	std::fstream input_file(input.string(), std::ios::in);
	std::fstream output_file(output.string(), std::ios::out);

	std::string curr_str;
	const std::regex re(R"([\s|,|.|;|:|(|)|<|>|=|+|\/|\[|\]|\'|\%|\-|\*|\?|\!|\@|\"]+)");
	
	while (std::getline(input_file, curr_str))
	{
		std::sregex_token_iterator it{curr_str.begin(), curr_str.end(), re, -1 };
		std::vector<std::string> tokenized{ it, {}};
		// Additional check to remove empty strings and one-char strings
		tokenized.erase(std::remove_if(tokenized.begin(), tokenized.end(),[](std::string const& s) {
			return s.size() == 0 || s.size() == 1;}),
		tokenized.end());

		for (auto& token : tokenized)
		{
			std::transform(token.begin(), token.end(), token.begin(), [](unsigned char c) { return std::tolower(c); });

			if (is_token_correct(token))
			{
				output_file << token << "\n";
			}
		}
	}

	return;
}

void make_tokenization(const fs::path& source, const fs::path& destination)
{
	unsigned long long filename_count = 0;
	for (auto& file : boost::make_iterator_range(fs::directory_iterator(source), {}))
	{
		tokenize_text(file, destination / (std::to_string(filename_count) + ".txt"));
		++filename_count;
	}

	return;
}

int main()
{
	fs::path work_folder = LR"(C:\Users\Alexey\Desktop\IS\2020-03-13)";
	fs::path parsed_data_folder_path = work_folder / parsed_data_folder;
	fs::path tokenized_folder_path = work_folder / tokenized_data_folder;

	utils::recreate_dir_safely(tokenized_folder_path);

	make_tokenization(parsed_data_folder_path, tokenized_folder_path);

	return 0;
}

\end{lstlisting}

\pagebreak

